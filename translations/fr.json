{
    "app_title": "Infrascale : Calculateur de GPU pour Inf√©rence LLM",
    "app_description": "Estimez le nombre de GPUs n√©cessaires pour servir un mod√®le de langage (LLM) en utilisant diff√©rentes strat√©gies de traitement par lots.",
    
    "section_workload": "1. D√©finir votre charge de travail",
    "text_workload_explanation": "Le nombre d'utilisateurs simultan√©s est le nombre d'utilisateurs que le syst√®me doit servir simultan√©ment. Par exemple, si vous avez donn√© acc√®s √† votre application √† 10000 utilisateurs, le nombre d'utilisateurs simultan√©s peut √™tre par exemple 10% d'entre eux, c'est-√†-dire 1000 utilisateurs simultan√©s.",
    "label_concurrent_users": "Nombre d'utilisateurs simultan√©s",
    "help_concurrent_users": "Le nombre total d'utilisateurs que le syst√®me doit servir simultan√©ment.",
    "label_throughput_per_user": "D√©bit par utilisateur (tokens/s)",
    "help_throughput_per_user": "La vitesse de g√©n√©ration de tokens requise pour chaque utilisateur.",
    "label_show_latency": "Afficher les calculs de latence",
    "text_min_ttft": "Le temps minimum jusqu'au premier token est de {min_ttft:.1f} s.",
    "label_target_ttft": "Temps cible jusqu'au premier token (s)",
    "help_target_ttft": "Temps maximum acceptable pour recevoir le premier token.",
    "warning_target_ttft_too_small": "‚ö†Ô∏è **Temps d'attente cible trop petit** : Le temps cible ({target_ttft} s) est trop petit. Envisagez de quantiser le mod√®le, d'utiliser des GPUs plus puissants, de r√©duire la longueur des prompts ou de r√©hausser le temps d'attente cible.",

    "section_model": "2. S√©lectionner votre mod√®le",
    "label_choose_model": "Choisir un mod√®le",
    "help_choose_model": "Le choix du mod√®le d√©termine automatiquement sa taille.",
    "label_quantization": "Pr√©cision de quantification",
    "help_quantization": "La pr√©cision num√©rique des poids du mod√®le. Une pr√©cision plus faible r√©duit l'utilisation m√©moire mais peut affecter la qualit√©.",
    "label_context_size": "Taille totale du contexte (tokens)",
    "help_context_size": "La longueur maximale de s√©quence (prompt + g√©n√©ration) √† traiter par requ√™te.",
    "label_advanced_prefill": "Avanc√© : Prefill vs Decode",
    "label_prompt_ratio": "Tokens de prompt (%)",
    "help_prompt_ratio": "Pourcentage de tokens en entr√©e (prefill) vs g√©n√©r√©s (decode).",
    "text_prompt_tokens": "Prompt : {prompt_tokens} tokens, G√©n√©r√©s : {generated_tokens} tokens",
    "text_effective_tokens": "Tokens effectifs pour le d√©bit : {effective_tokens:.0f}",
    
    "section_hardware": "3. Choisir le mat√©riel",
    "label_choose_gpu": "Choisir un type de GPU",
    "help_choose_gpu": "Le mat√©riel sur lequel le mod√®le sera ex√©cut√©.",
    
    "label_advanced_settings": "Param√®tres avanc√©s",
    "label_memory_overhead": "Surcharge m√©moire (%)",
    "help_memory_overhead": "M√©moire suppl√©mentaire pour le framework (vLLM, TGI, etc.), les kernels CUDA et les buffers.",
    
    "button_calculate": "üöÄ Calculer les GPUs requis",
    
    "error_config_load": "Erreur : Impossible de charger la configuration. Arr√™t.",
    "error_gpu_json_not_found": "Erreur : `db/gpu.json` introuvable. Veuillez le cr√©er.",
    "error_gpu_json_parse": "Erreur lors de l'analyse de `db/gpu.json` : {error}",
    "error_models_json_not_found": "Erreur : `db/models.json` introuvable. Veuillez le cr√©er.",
    "error_models_json_parse": "Erreur lors de l'analyse de `db/models.json` : {error}",
    
    "warning_model_too_large": "‚ö†Ô∏è **Attention** : Ce mod√®le ({model_mem:.1f} GB) ne tient pas sur un seul {gpu_name} ({gpu_vram} GB VRAM). Vous aurez besoin du parall√©lisme de mod√®le ou d'un GPU plus grand.",
    "error_impossible_config": "**Configuration impossible.** Le d√©bit demand√© par utilisateur est trop √©lev√© pour un seul GPU avec la configuration actuelle. Essayez de r√©duire la taille du batch, choisir un GPU plus puissant, ou r√©duire le d√©bit requis.",
    
    "success_gpus_required": "### Nombre estim√© de GPUs requis : **{gpu_count}**",
    "text_calculation_based_on": "Le calcul est bas√© sur le produit du nombre de GPUs par cluster et du nombre de clusters pour une taille de batch donn√©e :",
    "info_cluster_size": "**Nombre de GPUs par cluster :** `{constraint:.2f}` GPUs\n\nC'est le nombre de GPUs sur lesquels parall√©liser le d√©ploiement d'une instance du mod√®le.",
    "info_n_clusters": "**Nombre de clusters :** `{constraint:.2f}` Clusters\n\nC'est le nombre d'instances du mod√®le √† servir pour satisfaire la demande de d√©bit en tokens.",
    "info_batch_size": "**Taille du batch :** `{constraint:.2f}` requ√™tes\n\nC'est le nombre de requ√™tes √† traiter simultan√©ment pour une instance du mod√®le.",

    "tip_underutilized": "üí° **Conseil** : Vous utilisez moins d'un GPU de calcul. Envisagez d'utiliser le traitement continu pour une meilleure efficacit√© ou de servir plusieurs instances du mod√®le.",
    "warning_large_batch": "‚ö†Ô∏è **Grande taille de batch** : Les tailles de batch > 32 peuvent entra√Æner des rendements d√©croissants et une latence accrue. Envisagez de r√©duire si la latence est importante.",
    "warning_memory_bottleneck": "üìä Le goulot d'√©tranglement est la **m√©moire VRAM**. Les GPUs seront sous-utilis√©s en termes de calcul.",
    "warning_compute_bottleneck": "‚ö° Le goulot d'√©tranglement est la **vitesse de calcul (FLOPS)**. La VRAM sera sous-utilis√©e.",
    
    "heading_latency_estimates": "### Estimations de latence",
    "metric_ttft": "Temps jusqu'au premier token",
    "metric_time_per_token": "Temps par token",
    "metric_total_generation": "Temps total de g√©n√©ration",
    "warning_ttft_exceeded": "‚è±Ô∏è TTFT ({actual_ttft:.1f} ms) d√©passe la cible ({target_ttft} ms). Envisagez de r√©duire la taille du batch ou la longueur du prompt.",
    
    "units_ms": "{value:.1f} ms",
    "units_seconds": "{value:.2f} s"
  }