{
    "app_title": "Infrascale : Calculateur de GPU pour Inf√©rence LLM",
    "app_description": "Estimez le nombre de GPUs n√©cessaires pour servir un mod√®le de langage (LLM) en utilisant diff√©rentes strat√©gies de traitement par lots.",
    
    "section_workload": "1. D√©finir votre charge de travail",
    "label_concurrent_users": "Nombre d'utilisateurs simultan√©s",
    "help_concurrent_users": "Le nombre total d'utilisateurs que le syst√®me doit servir simultan√©ment.",
    "label_throughput_per_user": "D√©bit par utilisateur (tokens/s)",
    "help_throughput_per_user": "La vitesse de g√©n√©ration de tokens requise pour chaque utilisateur.",
    "label_show_latency": "Afficher les calculs de latence",
    "label_target_ttft": "Temps cible jusqu'au premier token (ms)",
    "help_target_ttft": "Temps maximum acceptable pour recevoir le premier token.",
    
    "section_model": "2. S√©lectionner votre mod√®le",
    "label_choose_model": "Choisir un mod√®le",
    "help_choose_model": "Le choix du mod√®le d√©termine automatiquement sa taille.",
    "label_quantization": "Pr√©cision de quantification",
    "help_quantization": "La pr√©cision num√©rique des poids du mod√®le. Une pr√©cision plus faible r√©duit l'utilisation m√©moire mais peut affecter la qualit√©.",
    "label_context_size": "Taille totale du contexte (tokens)",
    "help_context_size": "La longueur maximale de s√©quence (prompt + g√©n√©ration) √† traiter par requ√™te.",
    "label_advanced_prefill": "Avanc√© : Prefill vs Decode",
    "label_prompt_ratio": "Tokens de prompt (%)",
    "help_prompt_ratio": "Pourcentage de tokens en entr√©e (prefill) vs g√©n√©r√©s (decode).",
    "text_prompt_tokens": "Prompt : {prompt_tokens} tokens, G√©n√©r√©s : {generated_tokens} tokens",
    "text_effective_tokens": "Tokens effectifs pour le d√©bit : {effective_tokens:.0f}",
    
    "section_hardware": "3. Choisir le mat√©riel et la strat√©gie de traitement",
    "label_choose_gpu": "Choisir un type de GPU",
    "help_choose_gpu": "Le mat√©riel sur lequel le mod√®le sera ex√©cut√©.",
    "label_batch_size": "Taille maximale du batch",
    "help_batch_size": "Le nombre de requ√™tes utilisateur trait√©es simultan√©ment dans un seul batch.",
    "label_batching_strategy": "Strat√©gie de traitement par lots",
    "help_batching_strategy": "Diff√©rentes strat√©gies pour traiter les requ√™tes. Le traitement continu est le plus efficace.",
    "option_static_batching": "Traitement statique",
    "option_continuous_batching": "Traitement continu",
    "option_dynamic_batching": "Traitement dynamique",
    
    "label_advanced_settings": "Param√®tres avanc√©s",
    "label_memory_overhead": "Surcharge m√©moire (%)",
    "help_memory_overhead": "M√©moire suppl√©mentaire pour le framework (vLLM, TGI, etc.), les kernels CUDA et les buffers.",
    
    "button_calculate": "üöÄ Calculer les GPUs requis",
    
    "error_config_load": "Erreur : Impossible de charger la configuration. Arr√™t.",
    "error_gpu_json_not_found": "Erreur : `db/gpu.json` introuvable. Veuillez le cr√©er.",
    "error_gpu_json_parse": "Erreur lors de l'analyse de `db/gpu.json` : {error}",
    "error_models_json_not_found": "Erreur : `db/models.json` introuvable. Veuillez le cr√©er.",
    "error_models_json_parse": "Erreur lors de l'analyse de `db/models.json` : {error}",
    
    "warning_model_too_large": "‚ö†Ô∏è **Attention** : Ce mod√®le ({model_mem:.1f} GB) ne tient pas sur un seul {gpu_name} ({gpu_vram} GB VRAM). Vous aurez besoin du parall√©lisme de mod√®le ou d'un GPU plus grand.",
    "error_impossible_config": "**Configuration impossible.** Le d√©bit demand√© par utilisateur est trop √©lev√© pour un seul GPU avec la configuration actuelle. Essayez de r√©duire la taille du batch, choisir un GPU plus puissant, ou r√©duire le d√©bit requis.",
    
    "success_gpus_required": "### Nombre estim√© de GPUs requis : **{gpu_count}**",
    "text_calculation_based_on": "Le calcul est bas√© sur le maximum des deux contraintes suivantes :",
    "info_memory_constraint": "**Contrainte m√©moire :** `{constraint:.2f}` GPUs\n\nC'est le nombre minimum de GPUs requis pour stocker le mod√®le et ses caches.",
    "info_speed_constraint": "**Contrainte vitesse :** `{constraint:.2f}` GPUs\n\nC'est le nombre de GPUs requis pour satisfaire la demande de d√©bit en tokens.",
    
    "tip_underutilized": "üí° **Conseil** : Vous utilisez moins d'un GPU de calcul. Envisagez d'utiliser le traitement continu pour une meilleure efficacit√© ou de servir plusieurs instances du mod√®le.",
    "warning_large_batch": "‚ö†Ô∏è **Grande taille de batch** : Les tailles de batch > 32 peuvent entra√Æner des rendements d√©croissants et une latence accrue. Envisagez de r√©duire si la latence est importante.",
    "warning_memory_bottleneck": "üìä Le goulot d'√©tranglement est la **m√©moire VRAM**. Les GPUs seront sous-utilis√©s en termes de calcul.",
    "warning_compute_bottleneck": "‚ö° Le goulot d'√©tranglement est la **vitesse de calcul (FLOPS)**. La VRAM sera sous-utilis√©e.",
    
    "heading_latency_estimates": "### Estimations de latence",
    "metric_ttft": "Temps jusqu'au premier token",
    "metric_time_per_token": "Temps par token",
    "metric_total_generation": "Temps total de g√©n√©ration",
    "warning_ttft_exceeded": "‚è±Ô∏è TTFT ({actual_ttft:.1f} ms) d√©passe la cible ({target_ttft} ms). Envisagez de r√©duire la taille du batch ou la longueur du prompt.",
    
    "units_ms": "{value:.1f} ms",
    "units_seconds": "{value:.2f} s"
  }