{
    "app_title": "Infrascale: LLM Inference GPU Calculator",
    "app_description": "Estimate the number of GPUs required to serve a Large Language Model (LLM) using various batching strategies.",
    
    "section_workload": "1. Define Your Workload",
    "text_workload_explanation": "The number of concurrent users is the number of users that will be served simultaneously. For example, if you have given access to your app to 10000 users, the number of concurrent users may be for example 10% of them, i.e. 1000 concurrent users.",
    "label_concurrent_users": "Number of Concurrent Users",
    "help_concurrent_users": "The total number of users the system must serve simultaneously.",
    "label_throughput_per_user": "Throughput per User (tokens/s)",
    "help_throughput_per_user": "The required token generation speed for each user.",
    "label_show_latency": "Show latency calculations",
    "text_min_ttft": "The minimum time to the first token is {min_ttft:.1f} s.",
    "label_target_ttft": "Target Time to First Token (s)",
    "help_target_ttft": "Maximum acceptable time to receive the first token.",
    "warning_target_ttft_too_small": "‚ö†Ô∏è **Target Time to First Token too small** : The target ({target_ttft} s) is too small. Consider quantizing the model, using more powerful GPUs, reducing the prompt length, or increasing the target time to first token.",

    "section_model": "2. Select Your Model",
    "label_choose_model": "Choose a Model",
    "help_choose_model": "Choosing the model automatically determines its size.",
    "label_quantization": "Quantization Precision",
    "help_quantization": "The numerical precision of the model's weights. Lower precision reduces memory usage but may affect quality.",
    "label_context_size": "Total Context Size (tokens)",
    "help_context_size": "The maximum sequence length (prompt + generation) to be processed per request.",
    "label_advanced_prefill": "Advanced: Prefill vs Decode",
    "label_prompt_ratio": "Prompt tokens (%)",
    "help_prompt_ratio": "Percentage of tokens that are input (prefill) vs generated (decode).",
    "text_prompt_tokens": "Prompt: {prompt_tokens} tokens, Generated: {generated_tokens} tokens",
    "text_effective_tokens": "Effective tokens for throughput: {effective_tokens:.0f}",
    
    "section_hardware": "3. Choose Hardware",
    "label_choose_gpu": "Choose a GPU Type",
    "help_choose_gpu": "The hardware on which the model will run.",
    
    "label_advanced_settings": "Advanced Settings",
    "label_memory_overhead": "Memory Overhead (%)",
    "help_memory_overhead": "Extra memory for the framework (vLLM, TGI, etc.), CUDA kernels, and buffers.",
    
    "button_calculate": "üöÄ Calculate Required GPUs",
    
    "error_config_load": "Error: Failed to load configuration. Exiting.",
    "error_gpu_json_not_found": "Error: `db/gpu.json` not found. Please create it.",
    "error_gpu_json_parse": "Error parsing `db/gpu.json`: {error}",
    "error_models_json_not_found": "Error: `db/models.json` not found. Please create it.",
    "error_models_json_parse": "Error parsing `db/models.json`: {error}",
    
    "warning_model_too_large": "‚ö†Ô∏è **Warning**: This model ({model_mem:.1f} GB) won't fit on a single {gpu_name} ({gpu_vram} GB VRAM). You'll need model parallelism or a larger GPU.",
    "error_impossible_config": "**Impossible Configuration.** The requested throughput per user is too high for a single GPU with the current setup. Try reducing the batch size, choosing a more powerful GPU, or lowering the required throughput.",
    
    "success_gpus_required": "### Estimated Number of GPUs Required: **{gpu_count}**",
    "text_calculation_based_on": "The calculation is based on the product of the number of GPUs per cluster and the number of clusters for a given batch size:",
    "info_cluster_size": "**GPUs per cluster:** `{constraint:.2f}` GPUs\n\nThis is the number of GPUs to parallelise deployment of a single model instance on.",
    "info_n_clusters": "**Number of clusters:** `{constraint:.2f}` Clusters\n\nThis is the number of duplicated model instances to serve the number of concurrent users.",
    "info_batch_size": "**Batch size:** `{constraint:.2f}` requests\n\nThis is the number of requests to process simultaneously for a given model instance.",

    "tip_underutilized": "üí° **Tip**: You're using less than 1 GPU worth of compute. Consider using Continuous Batching for better efficiency or serving multiple model instances.",
    "warning_large_batch": "‚ö†Ô∏è **Large batch size**: Batch sizes > 32 may lead to diminishing returns and increased latency. Consider reducing if latency is important.",
    "warning_memory_bottleneck": "üìä The bottleneck is **VRAM Memory**. The GPUs will be underutilized in terms of computation.",
    "warning_compute_bottleneck": "‚ö° The bottleneck is **computation speed (FLOPS)**. The VRAM will be underutilized.",
    
    "heading_latency_estimates": "### Latency Estimates",
    "metric_ttft": "Time to First Token",
    "metric_time_per_token": "Time per Token",
    "metric_total_generation": "Total Generation Time",
    "warning_ttft_exceeded": "‚è±Ô∏è TTFT ({actual_ttft:.1f} ms) exceeds target ({target_ttft} ms). Consider reducing batch size or prompt length.",
    
    "units_ms": "{value:.1f} ms",
    "units_seconds": "{value:.2f} s"
  }